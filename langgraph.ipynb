{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import api_key\n",
    "from pydantic import SecretStr\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "api_key = SecretStr(input(\"Enter your OpenAI API Key:\"))\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: LLMs are not all knowing\n",
    "\n",
    "They have training cutoff dates, which means they cannot answer questions about events after that date.\n",
    "\n",
    "The model that we are using (gpt-4o-mini) has a training cutoff of October 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"What's on the front page of hacker news?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter tool calls\n",
    "\n",
    "Tool calls allow us to augment the LLM with current information, provide it tools to do meaningful work, or anything else that can be expressed with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def read_webpage(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a url's content and return it as a string\n",
    "    \"\"\"\n",
    "    print(\"Fetching content from\", url)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "tools = [read_webpage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from typing import cast\n",
    "\n",
    "call: AIMessage = cast(AIMessage, tool_model.invoke(\"what's on the front page of hacker news?\"))\n",
    "\n",
    "print(repr(call.content))\n",
    "print(call.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Machines - Can be thought of as a directed graph\n",
    "\n",
    "* States (Nodes)\n",
    "\t* The current _state_ of the graph. Dictates what action should be taken\n",
    "* Transitions (Edges)\n",
    "\t* The list of possible next values for state. Not all transitions are possible.\n",
    "\t\t* Eg: A traffic light has 3 transitions. Green => Yellow, Yellow => Red and Red => Green. Yellow => Green should be impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +-----------+         \n",
      "        | __start__ |         \n",
      "        +-----------+         \n",
      "              *               \n",
      "              *               \n",
      "              *               \n",
      "       +------------+         \n",
      "       | call_agent |         \n",
      "       +------------+         \n",
      "         .         .          \n",
      "       ..           ..        \n",
      "      .               .       \n",
      "+-------+         +---------+ \n",
      "| tools |         | __end__ | \n",
      "+-------+         +---------+ \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You must provide your answers in a format readable in a terminal. Do not provide any text other than the answer.\n",
    "    Your responses must not exceed 120 characters in width but can be multiple lines.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_agent(state: MessagesState):\n",
    "    response = tool_model.invoke([\n",
    "        (\"system\", system_prompt),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "def after_prompt_edge(state: MessagesState):\n",
    "    if cast(AIMessage, state[\"messages\"][-1]).tool_calls:\n",
    "        return tool_node.name\n",
    "    return END\n",
    "\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(call_agent)\n",
    "\n",
    "workflow.add_node(tool_node)\n",
    "\n",
    "workflow.add_edge(START, call_agent.__name__)\n",
    "workflow.add_edge(tool_node.name, call_agent.__name__)\n",
    "workflow.add_conditional_edges(call_agent.__name__, after_prompt_edge)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "app.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic / Tool Calling Demo\n",
    "\n",
    "Let's try that first example again, this time with our defined state machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from https://news.ycombinator.com/\n",
      "1. [Debugging: Indispensable rules for finding even the most elusive problems (2004)](https://dwheeler.com/essays/debugging-agans.html)  \n",
      "   Comments: [135](https://news.ycombinator.com/item?id=42682602)  \n",
      "\n",
      "2. [A Laptop Stand Made from a Single Sheet of Recycled Paper](https://www.core77.com/posts/134948/A-Laptop-Stand-Made-from-a-Single-Sheet-of-Recycled-Paper)  \n",
      "   Comments: [104](https://news.ycombinator.com/item?id=42662329)  \n",
      "\n",
      "3. [I created an open-source Hardware Hacking Wiki â€“ with tutorials for beginners](https://www.hardbreak.wiki)  \n",
      "   Comments: [16](https://news.ycombinator.com/item?id=42672821)  \n",
      "\n",
      "4. [Subway Stories](https://subwaystories.nyc/)  \n",
      "   Comments: [15](https://news.ycombinator.com/item?id=42684211)  \n",
      "\n",
      "5. [Lines of code that will beat A/B testing every time (2012)](https://stevehanov.ca/blog/index.php?id=132)  \n",
      "   Comments: [14](https://news.ycombinator.com/item?id=42650954)  \n",
      "\n",
      "6. [The Origins of Wokeness](https://paulgraham.com/woke.html)  \n",
      "   Comments: [164](https://news.ycombinator.com/item?id=42682305)  \n",
      "\n",
      "7. [The Curious Gems of the River Thames](https://www.atlasobscura.com/articles/thames-garnets-mudlark)  \n",
      "   Comments: [10](https://news.ycombinator.com/item?id=42684257)  \n",
      "\n",
      "8. [Entropy of a Large Language Model output](https://nikkin.dev/blog/llm-entropy.html)  \n",
      "   Comments: [35](https://news.ycombinator.com/item?id=42649315)  \n",
      "\n",
      "9. [Why is my CPU usage always 100%?](https://www.downtowndougbrown.com/2024/04/why-is-my-cpu-usage-always-100-upgrading-my-chumby-8-kernel-part-9/)  \n",
      "   Comments: [103](https://news.ycombinator.com/item?id=42649862)  \n",
      "\n",
      "10. [Humanize Healthcare Content as Wyndly (YC W21) 's first content hire](https://app.dover.com/apply/Wyndly/008f0389-988d-4b63-87c1-026b7b20c6fa/?rs=76643084)  \n",
      "    Comments: [0](https://news.ycombinator.com/item?id=42685628)  \n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Return the top 10 stories on hacker news, their titles, URLs, and comment URLs.\")\n",
    "]\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "messages.append(final_state['messages'][-1])\n",
    "\n",
    "print(final_state['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even ask follow up questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = input(\"What is your follow-up question?\")\n",
    "messages.append(HumanMessage(content=question))\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "messages.append(final_state['messages'][-1])\n",
    "\n",
    "print(\"You asked:\", question)\n",
    "print(final_state['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final demo\n",
    "\n",
    "Because the tool that we gave our agent was general, not specific, we are not limited to fetching results from hacker news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.invoke({\"messages\": [\n",
    "\tHumanMessage(content=\"What are the recent blog posts posted by Source Allies?\"),\n",
    "]})\n",
    "\n",
    "print(state['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
