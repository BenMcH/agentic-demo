{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import api_key\n",
    "from pydantic import SecretStr\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "api_key = SecretStr(input(\"Enter your OpenAI API Key:\"))\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\",\n",
    "                   api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: LLMs are not all knowing\n",
    "\n",
    "They have training cutoff dates, which means they cannot answer questions about events after that date.\n",
    "\n",
    "The model that we are using (gpt-4o-mini) has a training cutoff of October 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"What's onthe front page of hacker news?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter tool calls\n",
    "\n",
    "Tool calls allow us to augment the LLM with current information, provide it tools to do meaningful work, or anything else that can be expressed with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def read_webpage(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a url's content and return it as a string\n",
    "    \"\"\"\n",
    "    print(\"Fetching content from\", url)\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "\n",
    "tools = [read_webpage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_model = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from typing import cast\n",
    "\n",
    "call: AIMessage = cast(AIMessage, tool_model.invoke(\"what's on the front page of hacker news?\"))\n",
    "\n",
    "print(repr(call.content))\n",
    "print(call.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Machines - Can be thought of as a directed graph\n",
    "\n",
    "* States (Nodes)\n",
    "\t* The current _state_ of the graph. Dictates what action should be taken\n",
    "* Transitions (Edges)\n",
    "\t* The list of possible next values for state. Not all transitions are possible.\n",
    "\t\t* Eg: A traffic light has 3 transitions. Green => Yellow and Yellow => Green. Yellow => Green would be impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You must provide your answers in a format readable in a terminal. Do not provide any text other than the answer.\n",
    "    Your responses must not exceed 120 characters in width but can be multiple lines.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def call_agent(state: MessagesState):\n",
    "    response = tool_model.invoke([\n",
    "        (\"system\", system_prompt),\n",
    "        *state[\"messages\"]\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }\n",
    "\n",
    "\n",
    "def after_prompt_edge(state: MessagesState):\n",
    "    if cast(AIMessage, state[\"messages\"][-1]).tool_calls:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "workflow.add_node(\"prompt\", call_agent)\n",
    "\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"prompt\")\n",
    "workflow.add_edge(\"tools\", \"prompt\")\n",
    "workflow.add_conditional_edges(\"prompt\", after_prompt_edge)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic / Tool Calling Demo\n",
    "\n",
    "Let's try that first example again, this time with our defined state machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Return the top 10 stories on hacker news, their titles, URLs, and comment URLs.\")\n",
    "]\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "messages.append(final_state['messages'][-1])\n",
    "\n",
    "print(final_state['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even ask follow up questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = input(\"What is your follow-up question?\")\n",
    "messages.append(HumanMessage(content=question))\n",
    "\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": messages},\n",
    ")\n",
    "\n",
    "messages.append(final_state['messages'][-1])\n",
    "\n",
    "print(\"You asked:\", question)\n",
    "print(final_state['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final demo\n",
    "\n",
    "Because the tool that we gave our agent was general, not specific, we are not limited to fetching results from hacker news!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = app.invoke({\"messages\": [\n",
    "\tHumanMessage(content=\"What are the recent blog posts posted by Source Allies?\"),\n",
    "]})\n",
    "\n",
    "print(state['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
